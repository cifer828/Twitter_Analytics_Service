{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/local/opt/jenv/versions/openjdk64-1.8.0.242\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"twitter test\") \\\n",
    "    .config(\"spark.some.config.option\", \"SparkSessionExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext  # Note that spark is a SparkSession object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice \n",
    "# choice = 1 # single file\n",
    "#choice = 0 # all files\n",
    "choice = -1 # test file\n",
    "\n",
    "# Extract\n",
    "test_file = \"file:///Users/qiuchenzhang/Code/CMU/15619/Ying_Liu_Zhi_Zhu-S20/phase1/twitter/ETL/input/query2_ref.txt\"\n",
    "one_file = \"file:///Users/qiuchenzhang/Code/CMU/15619/Ying_Liu_Zhi_Zhu-S20/phase1/twitter/ETL/input//part-r-00000.gz\" \n",
    "all_files = \"gs://cmuccpublicdatasets/twitter/s20/*.gz\" \n",
    "\n",
    "filename = one_file if choice == 1 else all_files if choice == 0 else test_file\n",
    "df = spark.read.json(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform\n",
    "# 1. convert time to timetsamp\n",
    "# 2. concatenate hashtags with ,\n",
    "selectDf = df.select(col(\"id\").alias(\"tid\"),\n",
    "                     col(\"id_str\").alias(\"tid_str\"),\n",
    "                     unix_timestamp(col(\"created_at\"),\n",
    "                     \"EEE MMM dd HH:mm:ss ZZZZ yyyy\").alias(\"timestamp\"), \n",
    "                     col(\"text\").alias(\"content\"), \n",
    "                     col(\"in_reply_to_user_id\").alias(\"reply_to_uid\"),\n",
    "                     col(\"in_reply_to_user_id_str\").alias(\"reply_to_uid_str\"), \n",
    "                     col(\"user.id\").alias(\"sender_uid\"), \n",
    "                     col(\"user.id_str\").alias(\"sender_uid_str\"), \n",
    "                     col(\"user.screen_name\").alias(\"sender_screen_name\"),\n",
    "                     col(\"user.description\").alias(\"sender_description\"),\n",
    "                     col(\"retweeted_status.user.id\").alias(\"retweet_to_uid\"), \n",
    "                     col(\"retweeted_status.user.id_str\").alias(\"retweet_to_uid_str\"),\n",
    "                     col(\"retweeted_status.user.screen_name\").alias(\"retweet_to_uid_screen_name\"),\n",
    "                     col(\"retweeted_status.user.description\").alias(\"retweet_to_uid_description\"),\n",
    "                     concat_ws(\",\", col(\"entities.hashtags.text\")).alias(\"hashtags\"), \n",
    "                     col(\"lang\"))\n",
    "\n",
    "# filter out malformed tweets and tweets not using specific languages\n",
    "# 1. Cannot be parsed).alias(a JSON object\n",
    "# 2. Both id and id_str of the tweet object are missing or null\n",
    "# 3. Both id and id_str of the user object are missing or null\n",
    "# 4. created_at is missing or null\n",
    "# 5. text is missing or null or empty_string\n",
    "# 6. hashtag array missing or null or of length zero/empty\n",
    "filterDf = selectDf.filter(col(\"lang\").isin([\"ar\", \"en\", \"fr\", \"in\", \"pt\", \"es\", \"tr\", \"ja\"]) &\n",
    "                           (col(\"tid\").isNotNull() | col(\"tid_str\").isNotNull()) &\n",
    "                           (col(\"sender_uid\").isNotNull() | col(\"sender_uid_str\").isNotNull()) &\n",
    "                           col(\"timestamp\").isNotNull() &\n",
    "                           col(\"content\").isNotNull() &\n",
    "                           col(\"hashtags\").isNotNull()).where(length(col(\"hashtags\")) > 0)\n",
    "\n",
    "\n",
    "# combine user_id and user_str \n",
    "cleanDf = filterDf.withColumn(\"tid\", when(col(\"tid_str\").isNotNull(), col(\"tid_str\")).otherwise(col(\"tid\"))). \\\n",
    "                   withColumn(\"sender_uid\", when(col(\"sender_uid_str\").isNotNull(), col(\"sender_uid_str\")).otherwise(col(\"sender_uid\"))). \\\n",
    "                   withColumn(\"reply_to_uid\", when(col(\"reply_to_uid_str\").isNotNull(), col(\"reply_to_uid_str\")).otherwise(col(\"reply_to_uid\"))). \\\n",
    "                   withColumn(\"retweet_to_uid\", when(col(\"retweet_to_uid_str\").isNotNull(), col(\"retweet_to_uid_str\")).otherwise(col(\"retweet_to_uid\"))). \\\n",
    "                   drop(\"tid_str\", \"sender_uid_str\", \"reply_to_uid_str\", \"retweet_to_uid_str\")\n",
    "\n",
    "# tweetDf is for tweet_table\n",
    "tweetDf = cleanDf.drop(\"sender_screen_name\", \"sender_description\", \"retweet_to_uid_screen_name\", \"retweet_to_uid_description\")\n",
    "# userDf is for user_table\n",
    "senderDf = cleanDf.select(\"sender_uid\", \"sender_screen_name\", \"sender_description\", \"timestamp\")\n",
    "\n",
    "userMidDf = cleanDf.select(col(\"retweet_to_uid\").alias(\"uid\"), \n",
    "                           col(\"retweet_to_uid_screen_name\").alias(\"screen_name\"), \n",
    "                           col(\"retweet_to_uid_description\").alias(\"description\"), \n",
    "                           col(\"timestamp\")). \\\n",
    "                     filter(col(\"uid\").isNotNull()). \\\n",
    "                     union(senderDf)\n",
    "\n",
    "# find the latest information of each users\n",
    "# may cost a lot of time\n",
    "# only execute when running one file\n",
    "userDf = sc.parallelize([\"uid\", \"screen_name\", \"description\", \"lastestTime\"]).map(lambda x: (x, )).toDF()\n",
    "if choice != 0:\n",
    "    w = Window.partitionBy(col(\"uid\"))\n",
    "    userDf = userMidDf.withColumn(\"lastestTime\", max(\"timestamp\").over(w)). \\\n",
    "                              filter(col(\"lastestTime\") == col(\"timestamp\")). \\\n",
    "                              drop(\"timestamp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|timestamp|Value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "39092\n",
      "57585\n",
      "54060\n",
      "89\n",
      "root\n",
      " |-- uid: string (nullable = true)\n",
      " |-- screen_name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "userUnique = userDf.groupBy(col(\"uid\")).count().where(col(\"count\") > 1)\n",
    "\n",
    "# check if all timestamp are numeric\n",
    "userMidDf.select(\n",
    "  \"timestamp\",\n",
    "  col(\"timestamp\").cast(\"long\").isNotNull().alias(\"Value\")).where(~col(\"Value\")).show()\n",
    "\n",
    "print(filterDf.count())\n",
    "print(userMidDf.count())\n",
    "print(userDf.count())\n",
    "print(userUnique.count())\n",
    "\n",
    "userDf.registerTempTable(\"userDf\")\n",
    "userMidDf.registerTempTable(\"userMidDf\")\n",
    "cleanDf.registerTempTable(\"cleanDf\")\n",
    "spark.sql(\"select timestamp from userMidDf\")\n",
    "userMidDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "# Load to intermediate file\n",
    "# save to intermediate file, which dilimiter should use ???\n",
    "\n",
    "testOutput = \"file:///Users/qiuchenzhang/Code/CMU/15619/Ying_Liu_Zhi_Zhu-S20/phase1/twitter/ETL/output/raw2csv/testoutput/\"\n",
    "oneOutput = \"file:///Users/qiuchenzhang/Code/CMU/15619/Ying_Liu_Zhi_Zhu-S20/phase1/twitter/ETL/output/raw2csv/oneOutput/\"\n",
    "allOutput = \"file:///Users/qiuchenzhang/Code/CMU/15619/Ying_Liu_Zhi_Zhu-S20/phase1/twitter/ETL/output/raw2csv/allOutput/\"\n",
    "\n",
    "output = oneOutput if choice == 1 else allOutput if choice == 0 else testOutput\n",
    "\n",
    "tweetDf.write.mode(\"overwrite\").csv(output + \"tweetDf\", sep=\"⊢\", header=True ) # overwrite old result\n",
    "selectDf.write.mode(\"overwrite\").csv(output + \"selectDf\", sep=\"⊢\", header=True ) # overwrite old result\n",
    "filterDf.write.mode(\"overwrite\").csv(output + \"filterDf\", sep=\"⊢\", header=True ) # overwrite old result\n",
    "userMidDf.write.mode(\"overwrite\").csv(output + \"userMidDf\", sep=\"⊢\", header=True ) # overwrite old result\n",
    "userDf.write.mode(\"overwrite\").csv(output + \"userDf\", sep=\"⊢\", header=True ) # overwrite old result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
